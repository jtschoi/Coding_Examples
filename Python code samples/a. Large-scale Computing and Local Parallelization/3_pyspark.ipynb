{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACS 30123\n",
    "### Coding Section for Assignment 3\n",
    "### Professor Jon Clindaniel, TA Dhruval Bhatt \n",
    "### Submitted by Junho Choi\n",
    "\n",
    "I note that all of the code below should be run on the AWS EMR notebook, as instructed in Lab 6 by the Professor. I intentionally have not actually run the codes in this notebook as it was written locally; however, the code results will be discussed in the descriptive part of the submission (i.e., the file `junhoc_hw3.pdf`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A. Question 1-(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-1. Load data and install packages\n",
    "\n",
    "Firstly, let us load the necessary Amazon Customer Reviews data as `data`. Also, following the Lab 6 notebook, we install the necessary packages (`pandas`, `matplotlib`, and `seaborn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data load-in\n",
    "data = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet')\n",
    "\n",
    "## installing packages\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"seaborn\")\n",
    "sc.install_pypi_package(\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-2. Creating the necessary features\n",
    "\n",
    "In this part, I will create four additional variables to be added into the feature set. These are: `verified`, `body_wordcount`, `marketplace_feature`, and `pre2005`. I will explain each of them as we go through the below code.\n",
    "\n",
    "**Creating verified**\n",
    "\n",
    "The variable in `data` called `verified_purchase` is a binary variable that has either `Y` or `N` as possible values (without missing data). For this to be used as a part of the feature set, I will have to turn it into integer values. I therefore create `verified`, which equals to `1` if `verified_purchase` is `Y` (and to `0` if otherwise), using the below code, and add it to `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('verified', (data.verified_purchase == 'Y').cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating body_wordcount**\n",
    "\n",
    "`body_wordcount` is a variable derived from another variable `review_body`, which is the body of the sample review that was written. As its namesake, `body_wordcount` will count the number of words in the said review. The below code `import`s `pyspark.sql.functions` that is necessary to conduct the word count, create the word count, and add it to `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sparkfn\n",
    "\n",
    "data = data.withColumn('body_wordcount',\n",
    "    sparkfn.size(sparkfn.split(sparkfn.col('review_body'), ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating marketplace_feature**\n",
    "\n",
    "`marketplace_feature` is derived from `marketplace`, where the latter is the string variable to indicate the country in which the product was marketed. From my inspection, there were five countries (`US`, `DE` (Germany), `JP` (Japan), `FR` (France), and `UK`). I used string-indexing and one-hot-encoding to create `marketplace_feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspecting the possible marketplaces; none missing\n",
    "data.groupBy('marketplace').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing one-hot encoding and string-indexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "## building the encoder and indexer\n",
    "indexer = StringIndexer(inputCol='marketplace', outputCol='marketplace_numeric')\n",
    "onehot = OneHotEncoderEstimator(\n",
    "    inputCols=['marketplace_numeric'], outputCols=['marketplace_feature'])\n",
    "\n",
    "## applying and transforming the data\n",
    "data = indexer.fit(data).transform(data)\n",
    "data = onehot.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating pre2005**\n",
    "\n",
    "`pre2005` is derived from `year`, where the latter indicates the year in which the product was sold or marketed. From my inspection, there were 21 years possible (from 1995 to 2015, inclusive). `pre2005` equals to 1 if the `year` value is (inclusively) between 1995 and 2004, and equals to 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the pre2005 variable\n",
    "data = data.withColumn('pre2005', (data.year < 2005).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-3. Grouping the necessary features, and outcome variable creation\n",
    "\n",
    "We were told to add four features in addition to the `total_votes` feature which was used in Lab 6; I will now group `total_votes`, `marketplace_feature`, `pre2005`, `verified`, and `body_wordcount` as `feature_full`. In addition, I will have to create the outcome variable (i.e., `good_review` for being equal to or more than 4 stars). The below code will take care of these processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## bunching the features together\n",
    "feature_full = ['pre2005', 'verified', 'marketplace_feature'\n",
    "                'body_wordcount', 'total_votes']\n",
    "assemble = VectorAssembler(inputCols=feature_full, outputCol='feature_full')\n",
    "data = assemble.transform(data)\n",
    "\n",
    "## creating the good_review variable\n",
    "data = data.withColumn('good_review', (data.star_rating >= 4).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-4. Creating feature sets with one feature removed\n",
    "\n",
    "In 1-(b), we are told to produce a plot or a table to show the feature's potential contribution to the model. In order to do so, I will run the logisitic regression models with each of the 4 features (but at most 1) removed. To accomplish this, I create additional feature sets with at most one of the 4 features removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without pre2005\n",
    "feature_wo1 = ['verified', 'marketplace_feature',\n",
    "               'body_wordcount', 'total_votes']\n",
    "assemble_1 = VectorAssembler(inputCols=feature_wo1, outputCol='feature_wo1')\n",
    "data = assemble_1.transform(data)\n",
    "\n",
    "## without verified\n",
    "feature_wo2 = ['pre2005', 'marketplace_feature',\n",
    "               'body_wordcount', 'total_votes']\n",
    "assemble_2 = VectorAssembler(inputCols=feature_wo2, outputCol='feature_wo2')\n",
    "data = assemble_2.transform(data)\n",
    "\n",
    "## without marketplace_feature\n",
    "feature_wo3 = ['pre2005', 'verified',\n",
    "               'body_wordcount', 'total_votes']\n",
    "assemble_3 = VectorAssembler(inputCols=feature_wo3, outputCol='feature_wo3')\n",
    "data = assemble_3.transform(data)\n",
    "\n",
    "## without body_wordcount\n",
    "feature_wo4 = ['pre2005', 'marketplace_feature', 'verified',\n",
    "               'total_votes']\n",
    "assemble_4 = VectorAssembler(inputCols=feature_wo4, outputCol='feature_wo4')\n",
    "data = assemble_4.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Question 1-(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B-1. Running the logistic regression and returning the metrics\n",
    "\n",
    "Because the step is going to be rather repetitive (and potentially long), I will only demonstrate the code for the full model. I note that one can easily run the model with other feature sets (and produce the necessary metrics like AUC) by replacing `feature_full` with a relevant feature column (e.g., `feature_wo1` above). The results and comparisons are further described in `junhoc_hw3.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the train and test data\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=60615)\n",
    "\n",
    "## importing the logit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "## running the model\n",
    "lr = LogisticRegression(featuresCol='feature_full', labelCol='good_review')\n",
    "model_full = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summaries\n",
    "summ_train_full = model_full.summary\n",
    "summ_test_full = model_full.evaluate(test)\n",
    "\n",
    "## resulting AUC\n",
    "print(\"AUC, train, full model: \" + str(summ_train_full.areaUnderROC))\n",
    "print(\"AUC, test, full model: \", str(summ_test_full.areaUnderROC))\n",
    "print()\n",
    "\n",
    "## resulting accuracies\n",
    "print(\"Accuracy, train, full model: \" + str(summ_train_full.accuracy))\n",
    "print(\"Accuracy, test, full model: \", str(summ_test_full.accuracy))\n",
    "print()\n",
    "\n",
    "## resulting FPR\n",
    "print(\"False positive rate by label (Training):\")\n",
    "for i, rate in enumerate(summ_train_full.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "print()\n",
    "print(\"False positive rate by label (Testing):\")\n",
    "for i, rate in enumerate(summ_test_full.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "## resulting TPR\n",
    "print(\"\\nTrue positive rate by label (Training):\")\n",
    "for i, rate in enumerate(summ_train_full.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "    \n",
    "print(\"True positive rate by label (Testing):\")\n",
    "for i, rate in enumerate(summ_test_full.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B-1. Potential importance of feature, before running the model\n",
    "\n",
    "Instead of actually running the model to find the feature contributions or importances, we can also create tables and figures to see whether the feature has potential to be important or not. Therefore, I create cross-tabulations for the variables `pre2005`, `verified`, and `marketplace` against the variable `good_review` (the outcome variable). `marketplace_feature` is not used as it is in a one-hot encoded state and is less interpretable by humans. In addition, I create a figure plotting `good_review` against `body_wordcount` using a subsample (0.01%) of the entire dataset, to understand the two's relationship.\n",
    "\n",
    "**For pre2005**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.crosstab('good_review', 'pre2005').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For verified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.crosstab('good_review', 'verified').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For marketplace (marketplace_feature)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.crosstab('good_review', 'marketplace').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For body_wordcount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sampled_df = data.sample(fraction=0.0001).toPandas()\n",
    "sampled_df.plot.scatter('good_review', 'body_wordcount')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C. Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to \"balance\" the training data (in which there are many more `good_rating=1` observations than those with `good_rating=0`), we can use the `sampleBy` method as mentioned in the documentation. I actually create a `class` called `BinaryBalancer` that utilizes `sampleBy` method and can be fed into the `Pipeline` as well (for later purposes). I note that in creating this (and some of the transformation I define in Part D), I got a lot of help from looking at the following Towards Data Science post (link [here](https://towardsdatascience.com/pyspark-wrap-your-feature-engineering-in-a-pipeline-ee63bdb913)).\n",
    "\n",
    "**Describing the BinaryBalancer class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the necessary tool for building the class below\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.util import Identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryBalancer(Transformer):\n",
    "    '''\n",
    "    Transformer to return a \"balanced\" version of\n",
    "    the dataframe. Note that would work only if\n",
    "    the target variable to balance has TWO and only TWO\n",
    "    distinct values, and those values NEED TO BE EITHER\n",
    "    0 or 1.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, inputCol='good_review'):\n",
    "        '''\n",
    "        Initializer. Specify the input column (i.e., the\n",
    "        variable you want to balance by) in inputCol.\n",
    "        '''\n",
    "        \n",
    "        self.inputCol = inputCol\n",
    "        \n",
    "    def this():\n",
    "        '''\n",
    "        For identifying this class.\n",
    "        '''\n",
    "        \n",
    "        this(Identifiable.randomUID('binarybalancer'))\n",
    "    \n",
    "    def copy(extra):\n",
    "        '''\n",
    "        for retaining a copy.\n",
    "        '''\n",
    "        \n",
    "        defaultCopy(extra)\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        '''\n",
    "        actual balancing happens here; input your\n",
    "        target dataframe here.\n",
    "        '''\n",
    "        \n",
    "        ## counting and creating a table of counts\n",
    "        counter = df.count()\n",
    "        balancing = df.groupBy(\n",
    "            self.inputCol).count().collect()\n",
    "        \n",
    "        ## initializing the smaller count and its case\n",
    "        case = balancing[0][0]\n",
    "        smaller_num = balancing[0][1]\n",
    "        \n",
    "        ## finding the actual smaller count\n",
    "        for i, row in enumerate(balancing):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if row[1] < smaller_num:\n",
    "                    smaller_num = row[1]\n",
    "                    case = row[0]\n",
    "\n",
    "        ## target values are either 0 or 1\n",
    "        othercase = int(1 - case)\n",
    "        \n",
    "        ## creating a dictionary for the sampleBy method\n",
    "        balance_dict = {\n",
    "            case: 1,\n",
    "            othercase: (smaller_num / (counter-smaller_num))\n",
    "        }\n",
    "        \n",
    "        ## returning the balanced dataset, with the seed for\n",
    "        ## replicability\n",
    "        return df.sampleBy(self.inputCol, balance_dict,\n",
    "                           seed=60615)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the transformed (balanced) training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and test data split\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=60615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing the balancer\n",
    "balancer = BinaryBalancer(inputCol='good_review')\n",
    "\n",
    "## transforming; stored in train_bal\n",
    "train_bal = balancer.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting the results and comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the original train data\n",
    "train.groupBy('good_review').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the balanced train data\n",
    "train_bal.groupBy('good_review').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D. Question 3-(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-1. Pipelines: feature generation\n",
    "\n",
    "In order to create the pipelines, it is necessary that I create a ``Transformer`` version of the operations I have done in **Part A**. The process for `marketplace_feature` does not need to be altered (as it already uses `StringIndexer` and `OneHotEncoderEstimator` that can be fed into the `Pipeline`), but the other three variables' processes need some clean-up. The below code describes the necessary process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer for the three features: pre2005, body_wordcount, and verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeFeatureGenerator(Transformer):\n",
    "    '''\n",
    "    Transformer class for generating the following var.s:\n",
    "    - pre2005: =1 if year<2005, and =0 if not using the\n",
    "        year variable.\n",
    "    - body_wordcount: number of words in review_body\n",
    "    - verified =1 if verified_review='Y', and =0 if not\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 inputCols=['year', 'review_body', 'verified_review'],\n",
    "                 outputCols=['pre2005', 'body_wordcount', 'verified']):\n",
    "        '''\n",
    "        Initializer, for the input columns and output columns.\n",
    "        '''\n",
    "        \n",
    "        self.inputCol_1, self.inputCol_2, self.inputCol_3 = inputCols\n",
    "        self.outputCol_1, self.outputCol_2, self.outputCol_3 = outputCols\n",
    "        \n",
    "    def this():\n",
    "        '''\n",
    "        For identifying this class.\n",
    "        '''\n",
    "        \n",
    "        this(Identifiable.randomUID('binarybalancer'))\n",
    "    \n",
    "    def copy(extra):\n",
    "        '''\n",
    "        for retaining a copy.\n",
    "        '''\n",
    "        \n",
    "        defaultCopy(extra)\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        '''\n",
    "        Creating the 3 variables mentioned above.\n",
    "        '''\n",
    "        \n",
    "        ## creating pre2005\n",
    "        rtn_df = df.withColumn(self.outputCol_1,\n",
    "            sparkfn.when((df[self.inputCol_1] < 2005), 1).otherwise(0))\n",
    "        \n",
    "        ## creating body_wordcount\n",
    "        rtn_df = rtn_df.withColumn(self.outputCol_2,\n",
    "            sparkfn.size(sparkfn.split(sparkfn.col(self.inputCol_2), ' ')))\n",
    "\n",
    "        ## creating verified\n",
    "        rtn_df = rtn_df.withColumn(self.outputCol_3,\n",
    "            sparkfn.when((df[self.inputCol_3] == 'Y'), 1).otherwise(0))\n",
    "        \n",
    "        return rtn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-2. Creating the pipeline\n",
    "\n",
    "I assume that the Dataframe to be passed through the Pipeline described below already has the variable `good_review` as a column (as how we defined it in Lab 6). The Pipeline will accomplish the following steps:\n",
    "\n",
    "1. Firstly, the Pipeline will create the necessary four features (i.e., `pre2005`, `verified`, `body_wordcount`, and `marketplace_feature`).\n",
    "2. Using `VectorAssembler`, assemble the necessary features (including `total_votes` variable).\n",
    "3. Run logistic regression\n",
    "\n",
    "I emphasize that the below `pipeline` should be applied to a dataset that has _not_ yet generated the features `pre2005`, `verified`, `body_wordcount`, and `marketplace_feature` but has the outcome variable `good_review`. Also, if any training data is used, it should first be \"balanced\" (using the `BinaryBalancer` in Part C). I demonstrate this in the following sub-part D-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing Pipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the features verified, pre2005, and body_wordcount\n",
    "tfg = ThreeFeatureGenerator()\n",
    "\n",
    "## for creating marketplace_feature\n",
    "indexer = StringIndexer(inputCol='marketplace', outputCol='marketplace_numeric')\n",
    "onehot = OneHotEncoderEstimator(\n",
    "    inputCols=['marketplace_numeric'], outputCols=['marketplace_feature'])\n",
    "\n",
    "## for vector-assembling\n",
    "feature_full = ['pre2005', 'verified', 'marketplace_feature'\n",
    "                'body_wordcount', 'total_votes']\n",
    "assemble = VectorAssembler(inputCols=feature_full, outputCol='feature_full')\n",
    "\n",
    "## Logistic Regression step\n",
    "logit = LogisticRegression(featuresCol='feature_full', labelCol='good_review')\n",
    "\n",
    "## constructing the Pipeline\n",
    "pipeline = Pipeline(stages=[tfg, indexer, onehot, assemble, logit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-3. Demonstration of using the pipeline (without the cross-validation)\n",
    "\n",
    "I will now demonstrate, from data read-in to finally executing the `pipeline`, how to use the `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading in the data and creating the outcome variable\n",
    "df = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet')\n",
    "df = df.withColumn('good_review', (df.star_rating >= 4).cast(\"integer\"))\n",
    "\n",
    "## creating the training and testing data, then balancing the training data\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=60615)\n",
    "balancer = BinaryBalancer(inputCol='good_review')\n",
    "train_bal_df = balancer.transform(train_df)\n",
    "\n",
    "## using the pipeline to train the model\n",
    "## then getting the transformed test data\n",
    "logit_pipeline = pipeline.fit(train_bal_df)\n",
    "test_results = logit_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E. Question 4\n",
    "\n",
    "#### E-1. Function for pipeline-like structure and cross-validation\n",
    "\n",
    "Despite the `pipeline` above working well on its own, using it with `CrossValidator` seems to cause problems and errors as it lacks sufficient methods (and my coding skills are, unfortunately, not developed enough to add the right method). Instead, therefore, I will define a function that works like a `Pipeline` and also have the `CrossValidator` in the said function (to be denoted `PipeAndCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the necessary tools\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "import pyspark.ml.evaluation as evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline to be used in PipeAndCV\n",
    "pipeline_after_tfg = Pipeline(\n",
    "    stages=[indexer, onehot, assemble, logit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipeAndCV(df_use, pipeline_outer):\n",
    "    '''\n",
    "    A function for pipelining and cross-validating to help\n",
    "    find the best model\n",
    "    \n",
    "    Inputs:\n",
    "    - df_use: dataframe to be used for training the model\n",
    "    - pipeline_outer: pipeline to be used after manual cleaning\n",
    "    \n",
    "    Output:\n",
    "    - crossvalidated: cross-validated model by fitting the\n",
    "        df_use\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## manual cleansing\n",
    "    tfg = ThreeFeatureGenerator()\n",
    "    df_tfged = tfg.transform(df_use)\n",
    "    \n",
    "    ## setting up the evaluator\n",
    "    evaluator = evals.BinaryClassificationEvaluator(\n",
    "        metricName='areaUnderROC', labelCol='good_review')\n",
    "    \n",
    "    ## grid building\n",
    "    params = ParamGridBuilder()\n",
    "    params = params.addGrid(\n",
    "        logit.regParam, np.arange(0, 0.1, 0.01)).addGrid(\n",
    "            logit.elasticNetParam, [0, 1])\n",
    "    param_built = params.build()\n",
    "    \n",
    "    ## Cross validating\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline_outer,\n",
    "        estimatorParamMaps=param_built,\n",
    "        evaluator=evaluator, numFolds=5)\n",
    "    \n",
    "    ## Fitting the CVed model\n",
    "    models = cv.fit(df_tfged)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-2. Retrieving the 'best model'\n",
    "\n",
    "Here, I will demonstrate running the above function `PipeAndCV`. I note that I use the `train_bal_df` that was defined in sub-part D-3. Then, we can finally retrieve the \"best model\" in terms of maximized AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the PipeAndCV function\n",
    "models = PipeAndCV(train_bal_df, pipeline_after_tfg)\n",
    "\n",
    "## bm for best model\n",
    "bm = models.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-3. Transforming the test data\n",
    "\n",
    "I define another function, `TransformTestData`, for transforming the test data. The test data that I will use here is `test_df` that was also defined in sub-part D-3; note that `TransformTestData`, like `PipeAndCV`, has to additionally call upon `ThreeFeatureGenerator` which I had defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformTestData(df_use, model):\n",
    "    '''\n",
    "    A function for transforming dataframes according to the\n",
    "    specified model (notably, test datasets after models have\n",
    "    been fit with the training dataset)\n",
    "    \n",
    "    Inputs:\n",
    "    - df_use: dataframe to be used for transforming according to\n",
    "        the model specified by the input \"model\"\n",
    "    - model: for transforming the df_use after it's been\n",
    "        cleaned by pipeline_outer\n",
    "    \n",
    "    Output:\n",
    "    - transformed: transformed dataframe to be returned\n",
    "    \n",
    "    '''\n",
    "    ## manual cleansing (using tfg)\n",
    "    tfg = ThreeFeatureGenerator()\n",
    "    df_tfged = tfg.transform(df_use)\n",
    "    \n",
    "    ## transforming according to the model\n",
    "    transformed = model.transform(df_tfged)\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the above function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'transf' for transformed test dataset\n",
    "transf = TransformTestData(test_df, bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-4. Evaluations from fitting the test data\n",
    "\n",
    "As the question asks us to return the test data evaluations, I will return AUC, accuracy, true positive rates (for each label) and false positive rates (for each label). The results are discussed in `junhoc_hw4.pdf`.\n",
    "\n",
    "**AUC**\n",
    "\n",
    "AUC can be rather easily retrieved, using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the evaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(\n",
    "    metricName='areaUnderROC', labelCol='good_review')\n",
    "\n",
    "## saving the AUC\n",
    "auc_val = evaluator.evaluate(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, TPR, and FPR\n",
    "\n",
    "For this, I will try to manually calculate the metrics. In order to do so, I will have to keep track of whether the prediction was successful (i.e., the actual label and predicted label are equal to one another). I save this result in the column `sucess_pred`, and conduct the transformation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformation\n",
    "transf = transf.withColumn('success_pred',\n",
    "    sparkfn.when((transf['prediction'] == transf['good_review']), 1).otherwise(0))\n",
    "\n",
    "## can check by running the below code\n",
    "transf.select(['good_review', 'prediction', 'success_pred']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can calculate the test set prediction accuracy as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the sum of successfully predicted\n",
    "successful = transf.select(sparkfn.sum('success_pred')).collect()[0][0]\n",
    "\n",
    "## getting the total count\n",
    "total = transf.count()\n",
    "\n",
    "## saving the accuracy\n",
    "accuracy_val = successful / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar spirit, we can calculate TPR (true positive rate) for label of 1 (i.e., actual `good_review` value is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering by good_review==1\n",
    "transf_ones = transf.filter(\"good_review = 1\")\n",
    "\n",
    "## total number of actually being good_review=1\n",
    "total_ones = transf_ones.count()\n",
    "\n",
    "## getting the number of true positives (for good_review=1)\n",
    "true_pos_ones = transf_ones.select(\n",
    "    sparkfn.sum('success_pred')).collect()[0][0]\n",
    "\n",
    "## saving the TPR for good_review=1\n",
    "tpr_ones = true_pos_ones / total_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that TPR for label of 1 is equal to $1$ minus FPR for label of 0, since\n",
    "\n",
    "$$ TPR(1) = \\frac{TP(1)}{TP(1) + FN(1)} = \\frac{TN(0)}{TN(0) + FP(0)} = 1 - \\frac{FP(0)}{TN(0)+FP(0)} = 1 - FPR(0)$$\n",
    "\n",
    "where $TP$, $FN$, $TN$, $FP$ refer to true positive, false negative, true negative, and false positive. So, we can get the FPR for the label of 0 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the FPR for good_review=0\n",
    "fpr_zeros = 1 - tpr_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then, in the very similar spirit, output the TPR for label of 0 and FPR for label of 1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering by good_review==0\n",
    "transf_zeros = transf.filter(\"good_review = 0\")\n",
    "\n",
    "## total number of actually being good_review=0\n",
    "total_zeros = transf_zeros.count()\n",
    "\n",
    "## getting the number of true positives (for good_review=1)\n",
    "true_pos_zeros = transf_zeros.select(\n",
    "    sparkfn.sum('success_pred')).collect()[0][0]\n",
    "\n",
    "## saving the TPR for good_review=0\n",
    "tpr_zeros = true_pos_zeros / total_zeros\n",
    "\n",
    "## FPR for good_review=1\n",
    "fpr_ones = 1 - tpr_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the results\n",
    "\n",
    "Results can be output using the following series of `print`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset evaluation metrics are as follows:\")\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"Accuracy:\", round(accuracy_val, 4))\n",
    "print(\"AUC:     \", round(auc_val, 4))\n",
    "print()\n",
    "print(\"True positive rate:\")\n",
    "print(\"For good_review=1:\", round(tpr_ones, 4))\n",
    "print(\"For good_review=0:\", round(tpr_zeros, 4))\n",
    "print()\n",
    "print(\"False positive rate:\")\n",
    "print(\"For good_review=1:\", round(fpr_ones, 4))\n",
    "print(\"For good_review=0:\", round(fpr_zeros, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-5. Best model (hyper)parameters\n",
    "\n",
    "In case the best model (hyper)parameters are needed, I use the code below to output them (for `regParam` and `elasticNetParam`). I refer to the following link ([here](https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark)) of a StackOverflow post to print the below two hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regparam_val = bm.stages[-1]._java_obj.getRegParam()\n",
    "elnet_val = bm.stages[-1]._java_obj.getElasticNetParam()\n",
    "\n",
    "print(\n",
    "    \"Best regParam is {}, and best elasticNetParam is {}.\".format(\n",
    "        regparam_val, elnet_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F. Question 5\n",
    "\n",
    "For this question, it is only about running the above `PipeAndCV` multiple times with different numbers of cores specified, so there is nothing to further elaborate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
