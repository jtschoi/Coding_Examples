{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Section for Assignment 2\n",
    "### Professor Jon Clindaniel, TA Dhruval Bhatt\n",
    "### Submitted by Junho Choi\n",
    "\n",
    "This is the Coding Section accompanying the file `junhoc_hw2.pdf`, which contains more descriptive answers. This file contains codes that were used to conduct the assignment as well as related descriptions and annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Question 1-(a)\n",
    "\n",
    "#### A-1. Preparation for the scraping and database operations\n",
    "\n",
    "In this step, I import the necessary files for the web-scraping job as well as storage to the local `.db` file that I created (called `q1_storage.db`). In case you need to have access to the database I have created, I also upload the `q1_storage.db` file on GitHub as well. I note that there are four tables in the database, `parallel_book_info` and `parallel_books` (for storing the information collected using parallel strategy) and `serial_book_info` and `serial_books` (for that using serial strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parallel_book_info', 'parallel_books', 'serial_book_info', 'serial_books']\n"
     ]
    }
   ],
   "source": [
    "## importing necessary packages\n",
    "import pywren\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import dataset\n",
    "\n",
    "from datetime import datetime\n",
    "from pywren import default_executor as def_exec\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "## setting up the base url and connecting to the DB\n",
    "base_url = \"http://books.toscrape.com\"\n",
    "db = dataset.connect('sqlite:///q1_storage.db')\n",
    "\n",
    "## checking the tables\n",
    "print(db.tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-2. Code for scraping all pages\n",
    "In the parallelization step with `pywren`, I consider using parallelizing by pages on the catalogue. There are 50 pages in total and each page contains 20 items (books). The function below will scrape all the page URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages(start_url=base_url):\n",
    "    '''\n",
    "    This code will collect the list of all pages in the website.\n",
    "\n",
    "    Input:\n",
    "    - start_url (str): url to start scraping\n",
    "\n",
    "    Output:\n",
    "    - url_lst (list of str): list of page urls (in string format) \n",
    "\n",
    "    '''\n",
    "    url = start_url\n",
    "    url_list = [start_url]\n",
    "    start = True\n",
    "    while True:\n",
    "        if start:\n",
    "            print('Collecting the page URLs..')\n",
    "            start = False\n",
    "        r = requests.get(url)\n",
    "        html_soup = BSoup(r.text, 'html.parser')\n",
    "        next_a = html_soup.select('li.next > a')\n",
    "        if not next_a or not next_a[0].get('href'):\n",
    "            break\n",
    "        url = urljoin(url, next_a[0].get('href'))\n",
    "        url_list.append(url)\n",
    "\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-3. Code for scraping single book information\n",
    "\n",
    "The below code, given the book-information URL, will scrape the book information on that URL and return the said information in the form of a dictionary. Regarding how to retrieve the said book-information URL used as an input, I will describe this more in **part A-4** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_book_scraper(book_url):\n",
    "    '''\n",
    "    This code will scrape the book information (as well as the time and date\n",
    "    the scraping happened).\n",
    "    \n",
    "    Input:\n",
    "    - book_url (str): book information url used for scraping book information\n",
    "    \n",
    "    Output:\n",
    "    - book (dict): dictionary containing the book information; keys as\n",
    "        the column names in the database table, and values as the\n",
    "        corresponding information\n",
    "    '''\n",
    "    r = requests.get(book_url)\n",
    "    html_soup = BSoup(r.text, 'html.parser')\n",
    "\n",
    "    ## storing the info in the dictionary as in the example code\n",
    "    book = dict([])\n",
    "    book_and_last_seen = dict([])\n",
    "\n",
    "    ## book ID\n",
    "    book_id = urlparse(book_url).path.split('/')[2]\n",
    "    book['book_id'] = book_id\n",
    "    book_and_last_seen['book_id'] = book_id\n",
    "    \n",
    "    ## main part of the product page\n",
    "    main = html_soup.find(class_='product_main')\n",
    "    \n",
    "    ## from main, get title, price, stock\n",
    "    book['title'] = main.find('h1').get_text(strip=True)\n",
    "    book['price'] = main.find(class_='price_color').get_text(strip=True)\n",
    "    book['stock'] = main.find(class_='availability').get_text(strip=True)\n",
    "    book['rating'] = ' '.join(main.find(class_='star-rating').get(\n",
    "        'class')).replace('star-rating', '').strip()\n",
    "\n",
    "    ## getting the image source\n",
    "    book['image_source'] = html_soup.find(class_='thumbnail').img['src']\n",
    "\n",
    "    ## description\n",
    "    desc = html_soup.find(id='product_description')\n",
    "    book['description'] = ''\n",
    "    if desc:\n",
    "        book['description'] = desc.find_next_sibling('p').get_text(\n",
    "            strip=True)\n",
    "    \n",
    "    ## product information table\n",
    "    info_tbl = html_soup.find(\n",
    "        text='Product Information').find_next('table')\n",
    "    \n",
    "    for row in info_tbl.find_all('tr'):\n",
    "        header = row.find('th').get_text(strip=True)\n",
    "        header = re.sub('[^a-zA-Z]', '_', header)\n",
    "        value = row.find('td').get_text(strip=True)\n",
    "        book[header] = value\n",
    "\n",
    "    ## when did the book-info scraping finish approx.ly?\n",
    "    dt = datetime.now()\n",
    "    book_and_last_seen['last_seen'] = str(dt)\n",
    "\n",
    "    return book, book_and_last_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-4. Code for scraping books belonging to a single catalogue page\n",
    "\n",
    "In the below code, the scraper will go through each of the book pages (first by finding the URL for the book then using the above `single_book_scraper` function), get the relevant information in the form of dictionaries, and store (or \"upsert\") this information in the database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scraper(page_url, origin_page=base_url,\n",
    "                 database=db, books_table_name='serial_books',\n",
    "                 book_info_table_name='serial_book_info'):\n",
    "    '''\n",
    "    This function scrapes all the book information for the books\n",
    "    on a given page, and upserts it into the specified database's\n",
    "    table.\n",
    "    \n",
    "    Inputs:\n",
    "    - page_url (str): URL for the catalogue page\n",
    "    - origin_page (str): the website start page (for creating book URLs)\n",
    "    - database (dataset.database.Database): database object for connecting\n",
    "        to the .db file\n",
    "    - books_table_name (str): database table name to store the book ID and\n",
    "        last_seen variable\n",
    "    - book_info_table_name (str) : database table name to store book ID and\n",
    "        related book information\n",
    "    \n",
    "    Output: \n",
    "    - None; however, the book information will be\n",
    "        stored in the database and tables specified.\n",
    "    '''\n",
    "    ## checking if it is the first page (the URL is slightly d`ifferent)\n",
    "    first_page = True\n",
    "    if 'catalogue' in page_url:\n",
    "        first_page = False\n",
    "        \n",
    "    ## getting the beautifulsoup\n",
    "    r = requests.get(page_url)\n",
    "    html_soup = BSoup(r.text, 'html.parser')\n",
    "    \n",
    "    ## scraping each book information\n",
    "    for book in html_soup.select('article.product_pod'):\n",
    "        ## getting the book URL to send to the single_book_scraper\n",
    "        book_url = book.find('h3').find('a').get('href')\n",
    "        if first_page:\n",
    "            book_url = urljoin(origin_page, book_url)\n",
    "        else:\n",
    "            book_url = urljoin(origin_page, 'catalogue/'+book_url)\n",
    "            \n",
    "        ## book information scraped, in the form of dictionary\n",
    "        book_info, books = single_book_scraper(book_url)\n",
    "        \n",
    "        ## upserting to the relevant table in the database\n",
    "        database[book_info_table_name].upsert(book_info, ['book_id'])\n",
    "        database[books_table_name].upsert(books, ['book_id'])\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-5. Serially running the web scraping process\n",
    "\n",
    "Below code ensembles the functions above for conducting a serial web-scraping process (including the storage part), for comparing this with the parallelized case using `pywren`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_scraping(start_url=base_url, database=db,\n",
    "                    books_table_name='serial_books',\n",
    "                    book_info_table_name='serial_book_info'):\n",
    "    ## starting the time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## scraping the catalogue page URLs\n",
    "    page_urls = all_pages(start_url)\n",
    "    print('Collected all page URLs...')\n",
    "    \n",
    "    ## page scraping...\n",
    "    for i, page in enumerate(page_urls):\n",
    "        page_num = i+1\n",
    "        page_scraper(page, start_url, database,\n",
    "                     books_table_name, book_info_table_name)\n",
    "        \n",
    "        ## printing the progress every 10 pages\n",
    "        if page_num % 10 == 0:\n",
    "            print(\"Finished scraping and storing page {}...\".format(page_num))\n",
    "    \n",
    "    ## finished time output\n",
    "    t = time.time() - t0\n",
    "    t = round(t, 4)\n",
    "    print(\"Scraping finished; total elapsed time {} seconds.\".format(t))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us conduct the above-written serial scraping (and storing) code to see how long it takes.\n",
    "\n",
    "The result shows that the serial process took **550.3733 seconds** approximately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the page URLs..\n",
      "Collected all page URLs...\n",
      "Finished scraping and storing page 10...\n",
      "Finished scraping and storing page 20...\n",
      "Finished scraping and storing page 30...\n",
      "Finished scraping and storing page 40...\n",
      "Finished scraping and storing page 50...\n",
      "Scraping finished; total elapsed time 550.3733 seconds.\n"
     ]
    }
   ],
   "source": [
    "serial_scraping(base_url, db, 'serial_books', 'serial_book_info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the number of observations in the tables `serial_book_info` and `serial_books` just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(db['serial_book_info'].count(), db['serial_books'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check by printing out some rows of the tables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('book_id', 'a-light-in-the-attic_1000'), ('title', 'A Light in the Attic'), ('price', 'Â£51.77'), ('stock', 'In stock (22 available)'), ('rating', 'Three'), ('image_source', '../../media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg'), ('description', \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"), ('UPC', 'a897fe39b1053632'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£51.77'), ('Price__incl__tax_', 'Â£51.77'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (22 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'tipping-the-velvet_999'), ('title', 'Tipping the Velvet'), ('price', 'Â£53.74'), ('stock', 'In stock (20 available)'), ('rating', 'One'), ('image_source', '../../media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg'), ('description', '\"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s \"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s dresser and the two head for the bright lights of Leicester Square where they begin a glittering career as music-hall stars in an all-singing and dancing double act. At the same time, behind closed doors, they admit their attraction to each other and their affair begins. ...more'), ('UPC', '90fa61229261140a'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£53.74'), ('Price__incl__tax_', 'Â£53.74'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (20 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'soumission_998'), ('title', 'Soumission'), ('price', 'Â£50.10'), ('stock', 'In stock (20 available)'), ('rating', 'One'), ('image_source', '../../media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg'), ('description', 'Dans une France assez proche de la nÃ´tre, un homme sâ\\x80\\x99engage dans la carriÃ¨re universitaire. Peu motivÃ© par lâ\\x80\\x99enseignement, il sâ\\x80\\x99attend Ã\\xa0 une vie ennuyeuse mais calme, protÃ©gÃ©e des grands drames historiques. Cependant les forces en jeu dans le pays ont fissurÃ© le systÃ¨me politique jusquâ\\x80\\x99Ã\\xa0 provoquer son effondrement. Cette implosion sans soubresauts, sans vraie rÃ©volution, s Dans une France assez proche de la nÃ´tre, un homme sâ\\x80\\x99engage dans la carriÃ¨re universitaire. Peu motivÃ© par lâ\\x80\\x99enseignement, il sâ\\x80\\x99attend Ã\\xa0 une vie ennuyeuse mais calme, protÃ©gÃ©e des grands drames historiques. Cependant les forces en jeu dans le pays ont fissurÃ© le systÃ¨me politique jusquâ\\x80\\x99Ã\\xa0 provoquer son effondrement. Cette implosion sans soubresauts, sans vraie rÃ©volution, se dÃ©veloppe comme un mauvais rÃªve.Le talent de lâ\\x80\\x99auteur, sa force visionnaire nous entraÃ®nent sur un terrain ambigu et glissant ; son regard sur notre civilisation vieillissante fait coexister dans ce roman les intuitions poÃ©tiques, les effets comiques, une mÃ©lancolie fataliste.Ce livre est une saisissante fable politique et morale. ...more'), ('UPC', '6957f44c3847a760'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£50.10'), ('Price__incl__tax_', 'Â£50.10'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (20 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'a-light-in-the-attic_1000'), ('last_seen', '2020-05-20 19:55:27.972817')])\n",
      "\n",
      "OrderedDict([('book_id', 'tipping-the-velvet_999'), ('last_seen', '2020-05-20 19:55:28.610667')])\n",
      "\n",
      "OrderedDict([('book_id', 'soumission_998'), ('last_seen', '2020-05-20 19:55:28.997980')])\n",
      "\n",
      "OrderedDict([('book_id', 'sharp-objects_997'), ('last_seen', '2020-05-20 19:55:29.436873')])\n",
      "\n",
      "OrderedDict([('book_id', 'sapiens-a-brief-history-of-humankind_996'), ('last_seen', '2020-05-20 19:55:29.834484')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in db['serial_book_info'].find(_limit=3):\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "for i in db['serial_books'].find(_limit=5):\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-6. Page-level parallelization with pywren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the actual parallelization step, I will need to tweak the above `page_scraper` code (into `page_scraper2` described below) so that we can use the code with `pywren`. Instead of directly uploading and passing database as well as the table name, we return the acquired information as a list (so that it can later be inserted / updated / upserted and so forth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scraper2(page_url, origin_page=base_url):\n",
    "    '''\n",
    "    This function scrapes all the book information for the books\n",
    "    on a given page, and upserts it into the specified database's\n",
    "    table.\n",
    "    \n",
    "    Inputs:\n",
    "    - page_url (str): URL for the catalogue page\n",
    "    - origin_page (str): the website start page (for creating book URLs)\n",
    "\n",
    "    Output: \n",
    "    - collect_lst (list of dictionaries): scraped book information,\n",
    "        later to be used for storage\n",
    "    '''\n",
    "    \n",
    "    ## checking if it is the first page (the URL is slightly different)\n",
    "    first_page = True\n",
    "    if 'catalogue' in page_url:\n",
    "        first_page = False\n",
    "        \n",
    "    ## getting the beautifulsoup\n",
    "    r = requests.get(page_url)\n",
    "    html_soup = BSoup(r.text, 'html.parser')\n",
    "\n",
    "    collect_lst_book_info = []\n",
    "    collect_lst_books = []\n",
    "    \n",
    "    ## scraping each book information\n",
    "    for book in html_soup.select('article.product_pod'):\n",
    "        ## getting the book URL to send to the single_book_scraper\n",
    "        book_url = book.find('h3').find('a').get('href')\n",
    "        if first_page:\n",
    "            book_url = urljoin(origin_page, book_url)\n",
    "        else:\n",
    "            book_url = urljoin(origin_page, 'catalogue/'+book_url)\n",
    "            \n",
    "        ## book information scraped, in the form of dictionaries\n",
    "        book_info, books = single_book_scraper(book_url)\n",
    "        \n",
    "        ## appending the information to the lisst to return\n",
    "        collect_lst_book_info.append(book_info)\n",
    "        collect_lst_books.append(books)\n",
    "\n",
    "    return collect_lst_book_info, collect_lst_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code ensembles the functions above for conducting the scraping process, but in a parallelized manner using `pywren`. I note that it still collects the page URLs in a serial manner. Also, note that the entire storage process will take more time if we are \"upserting\" instead of using `insert_many`. The drawback of the latter case is that it cannot update existing rows, while faster than upserting or inserting row by row, according to the official `dataset` [website](https://dataset.readthedocs.io/en/latest/api.html#table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wren_scraping(start_url=base_url, inserting=True,\n",
    "                  database=db, books_table_name='parallel_books',\n",
    "                  book_info_table_name='parallel_book_info'):\n",
    "    '''\n",
    "    This function utilizes pywren (and therefore, AWS Lambda functions)\n",
    "    for parallelizing the web-scraping process. Also uploads the data\n",
    "    to the specified database and table.\n",
    "    \n",
    "    Inputs:\n",
    "    - start_url (str): URL for starting the scraping process\n",
    "    - inserting (boolean): if True, the process will bunch the scraped\n",
    "        book information into one list (of dictionaries) and\n",
    "        dataset-insert_many it; if False, the process will upsert,\n",
    "        one by one, the information in the list.\n",
    "    - database (dataset.database.Database): database object for connecting\n",
    "        to the .db file\n",
    "    - books_table_name (str): database table name to store the result about\n",
    "        the book ID and last_seen\n",
    "    - book_info_table_name (str): database table name to store the result about\n",
    "        the book ID and related book information\n",
    "\n",
    "    Output: \n",
    "    - None; however, the book information will be\n",
    "        stored in the database and tables specified.\n",
    "    '''\n",
    "\n",
    "    ## starting the time\n",
    "    t0 = time.time()\n",
    "\n",
    "    ## scraping the catalogue page URLs\n",
    "    page_urls = all_pages(start_url)\n",
    "    print('Collected all page URLs.\\n')\n",
    "\n",
    "    ## lambda function for the parallelized scraping process\n",
    "    fn = lambda x: page_scraper2(x, start_url)\n",
    "    \n",
    "    ## setting up the pywren\n",
    "    wrenexec = def_exec()\n",
    "    print('Starting the scraping process...')\n",
    "    \n",
    "    ## mapping with pywren; storing the results in a big list of dicts\n",
    "    futures = wrenexec.map(fn, page_urls)\n",
    "    results = pywren.get_all_results(futures)\n",
    "\n",
    "    ## merging into one list of dictionaries\n",
    "    ## \"results\" is a list of tuples of lists of dictionaries\n",
    "    result_lst_book_info = []\n",
    "    result_lst_books = []\n",
    "    for result in results:\n",
    "        result_lst_book_info += list(result[0])\n",
    "        result_lst_books += list(result[1])\n",
    "    print('Finished scraping.\\n')\n",
    "\n",
    "    print('Starting the storage process...')\n",
    "    if inserting:\n",
    "        length_books = len(result_lst_books)\n",
    "        length_book_info = len(result_lst_book_info)\n",
    "        \n",
    "        database[books_table_name].insert_many(\n",
    "            result_lst_books, chunk_size=length_books)\n",
    "        database[book_info_table_name].insert_many(\n",
    "            result_lst_book_info, chunk_size=length_book_info)\n",
    "    else:\n",
    "        for i, book_info in enumerate(result_lst_book_info):\n",
    "            database[book_info_table_name].upsert(\n",
    "                book_info, ['book_id'])\n",
    "            database[books_table_name].upsert(\n",
    "                result_lst_books[i], ['book_id'])\n",
    "    print('Finished the storage.\\n')\n",
    "\n",
    "    t = time.time() - t0\n",
    "    t = round(t, 4)\n",
    "    print(\"Process finished; total elapsed time {} seconds.\".format(t))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the `insert_many` case (i.e., inserting the rows en masse instead of upserting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the page URLs..\n",
      "Collected all page URLs.\n",
      "\n",
      "Starting the scraping process...\n",
      "Finished scraping.\n",
      "\n",
      "Starting the storage process...\n",
      "Finished the storage.\n",
      "\n",
      "Process finished; total elapsed time 34.3902 seconds.\n"
     ]
    }
   ],
   "source": [
    "wren_scraping(base_url, True, db,\n",
    "              'parallel_books', 'parallel_book_info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect the `upsert` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the page URLs..\n",
      "Collected all page URLs.\n",
      "\n",
      "Starting the scraping process...\n",
      "Finished scraping.\n",
      "\n",
      "Starting the storage process...\n",
      "Finished the storage.\n",
      "\n",
      "Process finished; total elapsed time 87.7527 seconds.\n"
     ]
    }
   ],
   "source": [
    "wren_scraping(base_url, False, db,\n",
    "              'parallel_books', 'parallel_book_info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can see that the parallel case with `insert_many` takes **34.3902 seconds** and that with `upsert` takes **87.7527 seconds**.\n",
    "\n",
    "Finally, let us check the number of observations in the tables `parallel_book_info` and `parallel_books` just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(db['parallel_book_info'].count(), db['parallel_books'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check by printing out some rows of the tables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('book_id', 'a-light-in-the-attic_1000'), ('title', 'A Light in the Attic'), ('price', 'Â£51.77'), ('stock', 'In stock (22 available)'), ('rating', 'Three'), ('image_source', '../../media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg'), ('description', \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"), ('UPC', 'a897fe39b1053632'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£51.77'), ('Price__incl__tax_', 'Â£51.77'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (22 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'tipping-the-velvet_999'), ('title', 'Tipping the Velvet'), ('price', 'Â£53.74'), ('stock', 'In stock (20 available)'), ('rating', 'One'), ('image_source', '../../media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg'), ('description', '\"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s \"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s dresser and the two head for the bright lights of Leicester Square where they begin a glittering career as music-hall stars in an all-singing and dancing double act. At the same time, behind closed doors, they admit their attraction to each other and their affair begins. ...more'), ('UPC', '90fa61229261140a'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£53.74'), ('Price__incl__tax_', 'Â£53.74'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (20 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'soumission_998'), ('title', 'Soumission'), ('price', 'Â£50.10'), ('stock', 'In stock (20 available)'), ('rating', 'One'), ('image_source', '../../media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg'), ('description', 'Dans une France assez proche de la nÃ´tre, un homme sâ\\x80\\x99engage dans la carriÃ¨re universitaire. Peu motivÃ© par lâ\\x80\\x99enseignement, il sâ\\x80\\x99attend Ã\\xa0 une vie ennuyeuse mais calme, protÃ©gÃ©e des grands drames historiques. Cependant les forces en jeu dans le pays ont fissurÃ© le systÃ¨me politique jusquâ\\x80\\x99Ã\\xa0 provoquer son effondrement. Cette implosion sans soubresauts, sans vraie rÃ©volution, s Dans une France assez proche de la nÃ´tre, un homme sâ\\x80\\x99engage dans la carriÃ¨re universitaire. Peu motivÃ© par lâ\\x80\\x99enseignement, il sâ\\x80\\x99attend Ã\\xa0 une vie ennuyeuse mais calme, protÃ©gÃ©e des grands drames historiques. Cependant les forces en jeu dans le pays ont fissurÃ© le systÃ¨me politique jusquâ\\x80\\x99Ã\\xa0 provoquer son effondrement. Cette implosion sans soubresauts, sans vraie rÃ©volution, se dÃ©veloppe comme un mauvais rÃªve.Le talent de lâ\\x80\\x99auteur, sa force visionnaire nous entraÃ®nent sur un terrain ambigu et glissant ; son regard sur notre civilisation vieillissante fait coexister dans ce roman les intuitions poÃ©tiques, les effets comiques, une mÃ©lancolie fataliste.Ce livre est une saisissante fable politique et morale. ...more'), ('UPC', '6957f44c3847a760'), ('Product_Type', 'Books'), ('Price__excl__tax_', 'Â£50.10'), ('Price__incl__tax_', 'Â£50.10'), ('Tax', 'Â£0.00'), ('Availability', 'In stock (20 available)'), ('Number_of_reviews', '0')])\n",
      "\n",
      "OrderedDict([('book_id', 'a-light-in-the-attic_1000'), ('last_seen', '2020-05-21 01:25:48.216768')])\n",
      "\n",
      "OrderedDict([('book_id', 'tipping-the-velvet_999'), ('last_seen', '2020-05-21 01:25:48.417247')])\n",
      "\n",
      "OrderedDict([('book_id', 'soumission_998'), ('last_seen', '2020-05-21 01:25:48.619428')])\n",
      "\n",
      "OrderedDict([('book_id', 'sharp-objects_997'), ('last_seen', '2020-05-21 01:25:48.821591')])\n",
      "\n",
      "OrderedDict([('book_id', 'sapiens-a-brief-history-of-humankind_996'), ('last_seen', '2020-05-21 01:25:49.025021')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in db['parallel_book_info'].find(_limit=3):\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "for i in db['parallel_books'].find(_limit=5):\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Question 2\n",
    "\n",
    "I note that I have collected the book descriptions in a separate `.csv` file (called `only_descs.csv`) which is also uploaded to the Github repository. This will be used to find the top 10 most frequent words. \n",
    "\n",
    "#### B-1. hw2_q2.py for conducting the search\n",
    "\n",
    "Below is the function to conduct the search for the top-ten most frequent words in the overall book descriptions. I note that the file `hw2_q2.py` is also separately uploaded to the Github repository as well.\n",
    "\n",
    "Note that in the very last `if` statement, the various `print` statements and timestamps are commented out. This is due to the fact that, if we run the below code in conjunction with EMR, it seems that the said lines cause errors. However, if testing locally, the `print` statements and timestamps will not cause any problems. If run locally using `mrjob` (using the command described in part **B-2**), the said lines can be un-commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw2_q2.py\n"
     ]
    }
   ],
   "source": [
    "%%file hw2_q2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRTopTenMostUsed(MRJob):\n",
    "    '''\n",
    "    Class for conducting the (steps of) mapreduce operations\n",
    "    to find the top ten count of most frequent words in the\n",
    "    book descriptions\n",
    "    \n",
    "    Input:\n",
    "    - MRJob: for conducting the mapreduce operation with\n",
    "        the mrjob package\n",
    "    \n",
    "    '''    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        '''\n",
    "        Mapper for preparing the word count\n",
    "        \n",
    "        Input:\n",
    "        - line (str): for creating the word and count pair\n",
    "        \n",
    "        Output (yield):\n",
    "        - tuple (word.lower(), 1): each word (even if redundant)\n",
    "            is paired with 1, so that the combiner / reducer can\n",
    "            compute the word counts\n",
    "        '''\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        '''\n",
    "        Combiner for the word count\n",
    "        \n",
    "        Input:\n",
    "        - word (str): lowercased word\n",
    "        - counts (int): should be incoming as 1\n",
    "        \n",
    "        Output (yield):\n",
    "        - tuple (word, sum(counts)): word and the associated\n",
    "            word count\n",
    "        '''\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        '''\n",
    "        Reducer for the word count\n",
    "        \n",
    "        Input:\n",
    "        - word (str): lowercased word\n",
    "        - counts (int): should be incoming as 1\n",
    "        \n",
    "        Output (yield):\n",
    "        - None: no keys for the next step.\n",
    "        - tuple (sum(counts), word): word count and the\n",
    "            associated word\n",
    "        '''\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_ten(self, _, word_count_pairs):\n",
    "        '''\n",
    "        Reducer for the finding the top ten cases\n",
    "        \n",
    "        Input:\n",
    "        - word_count_pairs (tuple): word count and the\n",
    "            associated word\n",
    "        \n",
    "        Output (yield):\n",
    "        - will output the top-nth word and its associated count,\n",
    "           where n = 1, 2, ..., 10\n",
    "        '''\n",
    "        sort_lst = sorted(word_count_pairs, reverse=True)\n",
    "        for i in range(10):\n",
    "            yield sort_lst[i][1], sort_lst[i][0]\n",
    "\n",
    "    def steps(self):\n",
    "        '''\n",
    "        Steps function for coordinating the entire\n",
    "        map reduce process.\n",
    "        \n",
    "        Output:\n",
    "        - rtn_lst: will contain the ending output of the\n",
    "            entire process.\n",
    "        '''\n",
    "        rtn_lst = [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_ten)\n",
    "        ]\n",
    "        return rtn_lst\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #print(\"Finding the top-ten most frequent words,\"\n",
    "    #      \" from 1st to 10th (with counts)\")\n",
    "    #print()\n",
    "    #t = time.time()\n",
    "    MRTopTenMostUsed.run()\n",
    "    #t1 = round(time.time() - t, 4)\n",
    "    #print()\n",
    "    #print(\"Computation time: {} seconds\".format(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B-2. How to run this from the command line\n",
    "\n",
    "If we are trying to run this in the command line **locally** (i.e., testing this locally before connecting to AWS EMR), we can type in the below command in the directory where both `hw2_q2_py` and `only_descs.csv` are in:\n",
    "\n",
    "`python hw2_q2.py < only_descs.csv > q2_local_mrjob.txt`\n",
    "\n",
    "and the output will be recorded in the text file `q2_local_mrjob.txt` (which is also uploaded to the Github, in the folder `Q2_outputs`).\n",
    "\n",
    "If we are trying to run this in conjunction with the **AWS EMR**, we can run the following command (in the same directory as mentioned above). Note that `.mrjob.conf` needs to be updated (with the right credentials).\n",
    "\n",
    "`python hw2_q2.py -r emr < only_descs.csv > q2_emr_mrjob.txt`\n",
    "\n",
    "and the output will be recorded in the text file `q2_emr_mrjob.txt` (also in the Github folder `Q2_outputs`).\n",
    "\n",
    "In my application of the codes, running this job locally took approximately **3.3766 seconds**; on the other hand, connecting this with AWS EMR actually took longer, with the computation time of approximately **7 minutes and 40 seconds (460 seconds)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B-3. Benchmark comparison by not using mrjob\n",
    "\n",
    "While it is not required by the question, I will compare the computation time with the simple way of finding the top-ten most used words without using `mrjob`. Below block of code is an example (benchmark) code. In this instance, the computation time is approximately **0.1406 seconds** (which is much, much faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "def not_using_mrjob(filename=\"only_descs.csv\"):\n",
    "    '''\n",
    "    Performs the top-ten frequent words finding operation\n",
    "    without using the mrjob package.\n",
    "    \n",
    "    Input:\n",
    "    - filename (str): file name of the csv file containing\n",
    "        book descriptions.\n",
    "    \n",
    "    Output:\n",
    "    - result (list of tuples): list of top-ten words with their\n",
    "        respective counts\n",
    "    - also prints out the computation time\n",
    "    \n",
    "    '''\n",
    "    ## timing starts\n",
    "    t = time.time()\n",
    "    \n",
    "    ## the file doesn't have any header row, so needs some processing\n",
    "    lines = pd.read_csv(\"only_descs.csv\")\n",
    "    lines = [lines.columns[0]] + list(lines.iloc[:, 0])\n",
    "    \n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    \n",
    "    ## cases to be stored in a dictionary\n",
    "    overall = dict([])\n",
    "    \n",
    "    for line in lines:\n",
    "        for word in WORD_RE.findall(line):\n",
    "            wordlower = word.lower()\n",
    "            if overall.get(wordlower) is None:\n",
    "                overall[wordlower] = 1\n",
    "            else:\n",
    "                overall[wordlower] += 1\n",
    "    \n",
    "    ## finding the top ten cases\n",
    "    lst_of_tuples = []\n",
    "    for i in list(overall.keys()):\n",
    "        lst_of_tuples.append((overall[i], i))\n",
    "        \n",
    "    ## sorting and returning the top ten cases\n",
    "    sorted_lst = sorted(lst_of_tuples, reverse=True)\n",
    "    result = sorted_lst[0:10]\n",
    "    \n",
    "    t1 = round(time.time()-t, 4)\n",
    "    print('Computation took {} seconds'.format(t1))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation took 0.1406 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13143, 'the'),\n",
       " (8693, 'and'),\n",
       " (7878, 'of'),\n",
       " (7085, 'a'),\n",
       " (6089, 'to'),\n",
       " (4333, 'in'),\n",
       " (3133, 'is'),\n",
       " (2510, 'her'),\n",
       " (2105, 'that'),\n",
       " (1999, 'with')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_using_mrjob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C. Question 3-(a)\n",
    "\n",
    "In this part, most of the codes will actually be launched from the Jupyter Notebook itself (unless noted otherwise). I also note that I heavily borrow from Professor Clindaniel's Lab 5 Part II codes.\n",
    "\n",
    "#### C-1. Setting up Kinesis and EC2 clients\n",
    "\n",
    "I borrow from Professor Clindaniel's code to set up the Kinesis and EC2 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "## setting up clients for kinesis and EC2 instance\n",
    "session = boto3.Session()\n",
    "kinesis = session.client('kinesis')\n",
    "ec2 = session.resource('ec2')\n",
    "ec2_client = session.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-2. Creating the Kinesis stream\n",
    "\n",
    "Since we only have one shard in our AWS Educate accounts, I made sure that all other Kinesis streams are terminated and deleted before running the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the stream\n",
    "stream_name = 'q3_stock_stream'\n",
    "response = kinesis.create_stream(StreamName = stream_name,\n",
    "                                 ShardCount = 1)\n",
    "\n",
    "## waiting until the stream creation process is actually finished\n",
    "waiter = kinesis.get_waiter('stream_exists')\n",
    "waiter.wait(StreamName=stream_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-3. Creating the EC2 instances\n",
    "\n",
    "I now create the EC2 instances. We will need two of them, one for the `producer` and the other for the `consumer`. I note that my permission key is also named `MACS_30123`, but for the security group (and its ID) I use different values (written below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my security-group-related values\n",
    "security_id, security_group = 'sg-0726a1a64e8f26e6d', 'junhoc-lab5-q3'\n",
    "\n",
    "# creating the instances\n",
    "instances = ec2.create_instances(\n",
    "    ImageId='ami-0915e09cc7ceee3ab', MinCount=1, MaxCount=2,\n",
    "    InstanceType='t2.micro', KeyName='MACS_30123',\n",
    "    SecurityGroupIds=[security_id], SecurityGroups=[security_group],\n",
    "    IamInstanceProfile={'Name': 'EMR_EC2_DefaultRole'}, )\n",
    "\n",
    "# making sure that the EC2 instances are created\n",
    "waiter = ec2_client.get_waiter('instance_running')\n",
    "waiter.wait(InstanceIds=[instance.id for instance in instances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-4. Script for the Producer instance\n",
    "\n",
    "Following the Lab 5 notebook and the code provided in the Assignment file, I set up the script for the Producer instance as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q3_producer.py\n"
     ]
    }
   ],
   "source": [
    "%%file q3_producer.py\n",
    "\n",
    "import boto3\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "## setup for the Kinesis client and the stream name\n",
    "stream_name = 'q3_stock_stream'\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "## code for generating stock prices (in USD terms)\n",
    "def getReferrer():\n",
    "    data = {}\n",
    "    now = datetime.datetime.now()\n",
    "    str_now = now.isoformat()\n",
    "    data['EVENT_TIME'] = str_now\n",
    "    data['TICKER'] = 'AAPL'\n",
    "    price = random.random() * 100\n",
    "    data['PRICE'] = round(price, 2)\n",
    "    return data\n",
    "\n",
    "## code for putting the generated stock price into Kinesis\n",
    "while True:\n",
    "    kinesis.put_record(StreamName=stream_name,\n",
    "                       Data=json.dumps(getReferrer()),\n",
    "                       PartitionKey=\"partitionkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-5. Script for the Consumer instance\n",
    "\n",
    "Following the Lab 5 notebook, I set up the script for the Consumer instance as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q3_consumer.py\n"
     ]
    }
   ],
   "source": [
    "%%file q3_consumer.py\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "## setup for the Kinesis client and the stream name\n",
    "stream_name = 'q3_stock_stream'\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "## shard iterator\n",
    "shard_it = kinesis.get_shard_iterator(\n",
    "    StreamName=stream_name, ShardId='shardId-000000000000',\n",
    "    ShardIteratorType='LATEST')[\"ShardIterator\"]\n",
    "\n",
    "while True:\n",
    "    ## Getting the stock price output\n",
    "    out = kinesis.get_records(ShardIterator=shard_it, Limit=1)\n",
    "    \n",
    "    ## all the records\n",
    "    for o in out['Records']:\n",
    "        stock_data = json.loads(o['Data'])\n",
    "\n",
    "    ## setting the latest values to be printed\n",
    "    time_info = stock_data['EVENT_TIME']\n",
    "    ticker_info = stock_data['TICKER']\n",
    "    price_info = stock_data['PRICE']\n",
    "        \n",
    "    print(\"Date and time: {}\".format(time_info))\n",
    "    print(\"Current price for {}: {}\".format(ticker_info, price_info))\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    shard_it = out['NextShardIterator']\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-6. Grabbing the DNS names of the instances (for convenience)\n",
    "\n",
    "I follow Professor Clindaniel's code for grabbing the DNS names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dns = [\n",
    "    instance.public_dns_name for instance in ec2.instances.all() \n",
    "    if instance.state['Name'] == 'running']\n",
    "\n",
    "code = ['q3_producer.py', 'q3_consumer.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-7. Finalizing the consumer and producer instances\n",
    "\n",
    "I also follow Professor Clindaniel's code for finalizing the establishment of consumer and producer instances. Note that there is a very minor change about using `enumerate` in the for loop and the `sudo pip install` statement for producer not requiring the `testdata` module. By running the below codes, the producer- and consumer-instances setup is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import time\n",
    "from scp import SCPClient\n",
    "\n",
    "## where my pem file is at\n",
    "my_pem_dir = 'C:/Users/Owner/MACS_30123.pem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Instance is Running producer.py\n",
      ".........................................\n",
      "Connect to Consumer Instance by running: ssh -i \"MACS_30123.pem\" ec2-user@ec2-54-210-250-53.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "## setting up the SSH\n",
    "ssh_producer, ssh_consumer = paramiko.SSHClient(), paramiko.SSHClient()\n",
    "\n",
    "# Initialization of SSH tunnels takes a bit of time; otherwise get connection error on first attempt\n",
    "time.sleep(5)\n",
    "\n",
    "# Install boto3 on each EC2 instance and Copy our producer/consumer code onto producer/consumer EC2 instances\n",
    "stdin, stdout, stderr = [[None, None] for i in range(3)]\n",
    "for instance, ssh in enumerate([ssh_producer, ssh_consumer]):\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(instance_dns[instance], username = 'ec2-user',\n",
    "                key_filename=my_pem_dir)\n",
    "    \n",
    "    with SCPClient(ssh.get_transport()) as scp:\n",
    "        scp.put(code[instance])\n",
    "\n",
    "    ## producer case\n",
    "    if instance == 0:\n",
    "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3\")\n",
    "    ## consumer case\n",
    "    else:\n",
    "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3\")\n",
    "\n",
    "# Block until Producer has installed boto3; then start running Producer script\n",
    "producer_exit_status = stdout[0].channel.recv_exit_status() \n",
    "if producer_exit_status == 0:\n",
    "    ssh_producer.exec_command(\"python %s\" % code[0])\n",
    "    print(\"Producer Instance is Running producer.py\\n.........................................\")\n",
    "else:\n",
    "    print(\"Error\", producer_exit_status)\n",
    "\n",
    "# Close ssh and show connection instructions for manual access to Consumer Instance\n",
    "ssh_consumer.close(); ssh_producer.close()\n",
    "\n",
    "print(\"Connect to Consumer Instance by running: ssh -i \\\"MACS_30123.pem\\\" ec2-user@%s\" % instance_dns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D. Question 3-(b)\n",
    "\n",
    "In this part, most of the codes will actually be launched from the Jupyter Notebook itself (unless noted otherwise). I also note that I heavily borrow from the Datacamp Assignment 1's part on using `SNS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-1. Creating the topic\n",
    "\n",
    "Following the instructions, I first create the topic `Price_Alert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sns:us-east-1:608865353210:Price_Alert\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# initialize boto3 client for sns\n",
    "session2 = boto3.Session()\n",
    "sns = session2.client('sns')\n",
    "\n",
    "# create the Price_Alert topic\n",
    "pa = 'Price_Alert'\n",
    "response = sns.create_topic(Name=pa)\n",
    "alert_arn = response['TopicArn']\n",
    "\n",
    "# checking the alert_arn\n",
    "print(alert_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-2. Subscribing my UChicago email\n",
    "\n",
    "I now subscribe my UChicago email (being `junhoc@uchicago.edu`) to the `Price_Alert` topic. I note that after having done so, I logged into my email account and confirmed my subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I use the previously-created alert_arn\n",
    "subscribe_email = sns.subscribe(\n",
    "    TopicArn=alert_arn, Protocol='email',\n",
    "    Endpoint='junhoc@uchicago.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-3. Initializing the Kinesis Stream\n",
    "\n",
    "Let us initialize the Kinesis Stream as in the Datacamp assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up the kinesis stream (consumer-side)\n",
    "stream_name = 'q3_stock_stream'\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "shard_it = kinesis.get_shard_iterator(\n",
    "    StreamName=stream_name, ShardId='shardId-000000000000',\n",
    "    ShardIteratorType='LATEST')[\"ShardIterator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-5. Getting the stock information and sending the price alert\n",
    "\n",
    "The below code accesses the Kinesis stream (from the consumer side). As soon as the price is below \\$3.00, it will send the alert via email, terminate or delete the related EC2 instances, SNS topic, and Kinesis Stream. I have posted the screenshot of the price alert email I received in the `junhoc_hw2.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date and time: 2020-05-19T22:12:28.213621\n",
      "Current price for AAPL: 1.77\n",
      "\n",
      "\n",
      "Waiting for the EC2 instances to be terminated...\n",
      "EC2 instances successfully terminated.\n",
      "\n",
      "\n",
      "Waiting for the topic to be deleted...\n",
      "Price_Alert topic has been successfully deleted.\n",
      "\n",
      "\n",
      "Waiting for the stream to be deleted...\n",
      "Kinesis stream successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    ## Getting the stock price output\n",
    "    out = kinesis.get_records(ShardIterator=shard_it, Limit=1)\n",
    "    \n",
    "    ## the most recent record\n",
    "    for o in out['Records']:\n",
    "        stock_data = json.loads(o['Data'])\n",
    "\n",
    "    ## setting the latest values to be printed\n",
    "    time_info = stock_data['EVENT_TIME']\n",
    "    ticker_info = stock_data['TICKER']\n",
    "    price_info = stock_data['PRICE']\n",
    "        \n",
    "    shard_it = out['NextShardIterator']\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ## see part C-1. for \"ec2_client\" (EC2 client)\n",
    "    ## see part C-3. for \"instances\" (storing EC2 instances)\n",
    "    if price_info < 3.0:\n",
    "        ## printing the stock information, just in case\n",
    "        print(\"Date and time: {}\".format(time_info))\n",
    "        print(\"Current price for {}: {}\".format(ticker_info, price_info))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        ## sending the price alert\n",
    "        msg = \"Price for {} is below $3.00; currently at ${}\".format(\n",
    "            ticker_info, price_info)\n",
    "        sbj = \"Urgent price alert for {}\".format(ticker_info)\n",
    "        sns.publish(TopicArn=alert_arn, Subject=sbj, Message=msg)\n",
    "            \n",
    "        ## terminating the EC2 instances\n",
    "        ec2_client.terminate_instances(\n",
    "            InstanceIds=[instance.id for instance in instances])\n",
    "        print(\"Waiting for the EC2 instances to be terminated...\")\n",
    "        \n",
    "        ## waiting for the EC2 instances to terminate\n",
    "        waiter = ec2_client.get_waiter('instance_terminated')\n",
    "        waiter.wait(InstanceIds=[instance.id for instance in instances])\n",
    "        print(\"EC2 instances successfully terminated.\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        ## deleting the SNS topic\n",
    "        sns.delete_topic(TopicArn=alert_arn)\n",
    "        print(\"Waiting for the topic to be deleted...\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        ## making sure that the SNS topic for Price Alert no longer exists\n",
    "        ## I checked that there are no \"waiters\" for SNS\n",
    "        lst_of_arns = list(pd.DataFrame(\n",
    "            sns.list_topics()['Topics'])['TopicArn'])\n",
    "        if alert_arn not in lst_of_arns:\n",
    "            print(\"{} topic has been successfully deleted.\".format(pa))\n",
    "        else:\n",
    "            print(\"Error in deleting the topic {}\".format(pa))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        ## Deleting the Kinesis Stream\n",
    "        ## \"stream_name\" was defined in Part C-2.\n",
    "        try:\n",
    "            response = kinesis.delete_stream(StreamName=stream_name)\n",
    "        except kinesis.exceptions.ResourceNotFoundException:\n",
    "            pass\n",
    "        \n",
    "        ## making sure the stream is deleted\n",
    "        print(\"Waiting for the stream to be deleted...\")\n",
    "        waiter = kinesis.get_waiter('stream_not_exists')\n",
    "        waiter.wait(StreamName='test_stream')\n",
    "        print(\"Kinesis stream successfully deleted.\")\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
